{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAKE NEWS DETECTOR - FEATURE ENGINEER (TFIDF AND COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF and CountVectorizer are techniques used for feature engineering in Natural Language Processing (NLP) tasks, which include Machine Learning tasks. In the context of fake news detection, both TF-IDF and CountVectorizer are employed to convert textual data into numerical representations that machine learning models can work with. These techniques are not directly related to deep learning tasks; instead, they are more commonly used in traditional machine learning algorithms.\n",
    "\n",
    "CountVectorizer converts a collection of text documents to a matrix of word counts, where each row corresponds to a document and each column corresponds to a unique word in the dataset. This matrix represents the frequency of each word's occurrence in each document. Multinomial Naive Bayes (MultinomialNB) is a machine learning algorithm that's often used in combination with CountVectorizer for classification tasks like fake news detection. It's particularly suitable for text classification problems.\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is another method that converts text data into numerical features. It considers not only the frequency of words in a document but also their importance relative to the entire dataset. Words that appear frequently in a specific document but rarely in others are considered more important. Like CountVectorizer, TF-IDF is used in conjunction with classifiers like MultinomialNB for classification tasks.\n",
    "\n",
    "F-IDF and CountVectorizer, when used in combination with classifiers like MultinomialNB, are essential tools for feature engineering and machine learning in the realm of fake news detection and text classification tasks. They help in transforming raw text data into structured numerical data that machine learning algorithms can analyze and make predictions on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/luisblazquezm/feature_enginering/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/luisblazquezm/feature_enginering/lib/python3.10/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: click in /home/luisblazquezm/feature_enginering/lib/python3.10/site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: tqdm in /home/luisblazquezm/feature_enginering/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: joblib in /home/luisblazquezm/feature_enginering/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: scikit-learn in /home/luisblazquezm/feature_enginering/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/luisblazquezm/feature_enginering/lib/python3.10/site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/luisblazquezm/feature_enginering/lib/python3.10/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/luisblazquezm/feature_enginering/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/luisblazquezm/feature_enginering/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk\n",
    "! pip install scikit-learn\n",
    "\n",
    "#### Others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#### Scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "\n",
    "#### NLTK\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH = \"./data/feature_engineering/\"\n",
    "TRAIN_RATIO = 0.70\n",
    "TEST_RATIO = 0.30\n",
    "MAX_FEATURES_VECTORIZER= 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_data = pd.read_csv(DATA_BASE_PATH + 'train.csv')\n",
    "test_data = pd.read_csv(DATA_BASE_PATH + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet involves data preprocessing steps.\n",
    "\n",
    "First of all, we must remove the rows with missing values (NaN) from the train_data dataset. \n",
    "\n",
    "After dropping rows, the indices of the remaining rows may become non-contiguous by resetting the index of the DataFrame to ensure continuous and sequential indexing. This results in an updated DataFrame with a reset index, where the previous index values are moved to a new column, and a new sequential index is assigned to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN\n",
    "train_data = train_data.dropna()\n",
    "train_data.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step to continue preprocessing is to prepare input features and targe labels by creating two dataframes, one of them leaving only the input features for model training, and the other with the 'label' column is assigned to the variable y_train, representing the target labels corresponding to the input features in x_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target column (Y) and input features (X)\n",
    "x_train = train_data.drop('label',axis =1)\n",
    "y_train = train_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet initializes a PorterStemmer (from the Natural Language Toolkit (NLTK) library is initialized. The stemmer will be used to reduce words to their root form.) for text stemming and creates empty lists corpus and words. It iterates through each title in the DataFrame, removing non-alphanumeric characters, converting to lowercase, and splitting into words. The words are then stemmed and stopwords are removed, resulting in preprocessed sentences added to the corpus list and individual stemmed words to the words list. This process prepares the text data for analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and preprocessing\n",
    "def preprocessing(data):\n",
    "    ps = PorterStemmer()\n",
    "    words = []\n",
    "    corpus = []\n",
    "    for i in range(0,len(data)):\n",
    "        review = re.sub('[^a-zA-Z0-9]',' ',train_data['title'][i])\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "        statements = ' '.join(review)\n",
    "        corpus.append(statements)\n",
    "        words.append(review)\n",
    "\n",
    "    return corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocessing(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code segment involves the creation of two text vectorizers, CountVectorizer and TfidfVectorizer. The CountVectorizer is configured to consider up to 5000 features (unique words) and include n-grams of size 1 to 3. It then transforms the preprocessed text data stored in the corpus list into a numeric matrix X_count using the .fit_transform() method. \n",
    "\n",
    "Similarly, the TfidfVectorizer is set with the same parameters, and the transformed matrix is stored in X_tfidf. These matrices represent the tokenized and vectorized versions of the text data for further analysis or machine learning modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare vectorizers (CountVectorizer)\n",
    "count_vectorizer_handler = CountVectorizer(max_features=MAX_FEATURES_VECTORIZER,ngram_range=(1,3))\n",
    "X_count = count_vectorizer_handler.fit_transform(corpus).toarray()\n",
    "\n",
    "# Prepare vectorizers (TFIDFVectorizer)\n",
    "tfidf_handler = TfidfVectorizer(max_features=MAX_FEATURES_VECTORIZER,ngram_range=(1,3))\n",
    "X_tfidf = tfidf_handler.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the provided context, both CountVectorizer and TfidfVectorizer are used for text vectorization, but they differ in how they assign weights to words in the text data.\n",
    "\n",
    "- CountVectorizer: This vectorizer counts the frequency of each word in the corpus and represents the text data as a matrix where each row corresponds to a document and each column corresponds to a unique word in the entire corpus. The values in the matrix are the raw word counts. This approach is useful for capturing the importance of words based on their frequency, but it may not handle the varying importance of words across different documents.\n",
    "\n",
    "- TfidfVectorizer (Term Frequency-Inverse Document Frequency): This vectorizer also counts the frequency of words, but it also takes into account the rarity of words in the entire corpus. It assigns higher weights to words that are frequent in a document but rare in other documents, as these words are assumed to carry more informative content. This helps mitigate the issue of common words dominating the analysis. The resulting matrix contains normalized values representing the TF-IDF scores for each word in each document.\n",
    "\n",
    "In summary, while both vectorizers transform text data into numerical representations, TfidfVectorizer additionally considers the significance of words within a specific document and across the entire corpus, making it often more suitable for tasks like document classification or information retrieval where term importance matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split for CountVectorizer\n",
    "X_train_count,X_test_count,Y_train_count,Y_test_count = train_test_split(X_count,y_train,test_size=TEST_RATIO,random_state = 27)\n",
    "\n",
    "# Split for TFIDFVectorizer\n",
    "X_train_tfidf,X_test_tfidf,Y_train_tfidf,Y_test_tfidf = train_test_split(X_tfidf,y_train,test_size=TEST_RATIO,random_state = 27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet uses the get_params() method on a CountVectorizer object named count to retrieve its current configuration settings. The output shows a dictionary containing various parameters and their current values that define how the CountVectorizer is functioning. Some of the notable parameters and their explanations are as follows:\n",
    "\n",
    "- analyzer: Specifies whether words should be treated as \"word\" units.\n",
    "- binary: Whether the word counts should be binary (True if the word is present, False if not). In this cse, it is not binary.\n",
    "- max_features: The maximum number of features (words) to include in the vocabulary. In this case, 5000\n",
    "- ngram_range: The range of n-grams to consider (in this case, unigrams, bigrams, and trigrams). In this case, 1 to 3.\n",
    "- lowercase: Whether to convert all words to lowercase before counting.\n",
    "- max_df: The threshold for ignoring words that appear in a high proportion of documents.\n",
    "- min_df: The threshold for ignoring words that appear in a low proportion of documents.\n",
    "- stop_words: A list of words to be ignored (common words like \"and\", \"the\", etc.). No stopwords ignored here, it was made before on the stemming.\n",
    "- strip_accents: How to handle accents (if any) in words. No accents striped here.\n",
    "- token_pattern: The regular expression pattern for identifying tokens (words).\n",
    "- vocabulary: A mapping of words to feature indices.\n",
    "\n",
    "These parameters control how the CountVectorizer preprocesses and constructs the feature vectors from the text data. By inspecting these parameters, you can understand how the vectorization is being performed and potentially fine-tune the settings to better suit your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': 500,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 3),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer_handler.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet creates a DataFrame named df_count from the feature matrix X_train_count generated by the CountVectorizer. Each row of the DataFrame corresponds to a document (text sample), and each column corresponds to a unique word (feature) present in the vocabulary created by the CountVectorizer. The columns are labeled with the words themselves using the tfidf.get_feature_names() method. \n",
    "\n",
    "This DataFrame allows us to see the frequency counts of each word (feature) in the documents. The .head() method is used to display the first few rows of the DataFrame, providing an initial glimpse of the word frequency distribution across the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>20</th>\n",
       "      <th>2016</th>\n",
       "      <th>access</th>\n",
       "      <th>accus</th>\n",
       "      <th>act</th>\n",
       "      <th>ad</th>\n",
       "      <th>...</th>\n",
       "      <th>worker</th>\n",
       "      <th>world</th>\n",
       "      <th>world war</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "      <th>york</th>\n",
       "      <th>york time</th>\n",
       "      <th>young</th>\n",
       "      <th>zika</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  10  100  11  20  2016  access  accus  act  ad  ...  worker  world  \\\n",
       "0    0   0    0   0   0     0       0      0    0   0  ...       0      0   \n",
       "1    0   0    0   0   0     0       0      0    0   0  ...       0      0   \n",
       "2    0   0    0   0   0     0       0      0    0   0  ...       0      0   \n",
       "3    0   0    0   0   0     0       0      0    0   0  ...       0      0   \n",
       "4    0   0    0   0   0     0       0      0    0   0  ...       0      0   \n",
       "\n",
       "   world war  would  year  year old  york  york time  young  zika  \n",
       "0          0      0     0         0     0          0      0     0  \n",
       "1          0      0     0         0     0          0      0     0  \n",
       "2          0      0     0         0     0          0      0     0  \n",
       "3          0      0     0         0     1          1      0     0  \n",
       "4          0      0     0         0     1          1      0     0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count = pd.DataFrame(X_train_count,columns = tfidf_handler.get_feature_names_out())\n",
    "df_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use  two instances of the MultinomialNB classifier to create the predictors. \n",
    "\n",
    "This classifier is used for text classification tasks, particularly suited for multinomially distributed data, such as word counts in text data. The parameter alpha is set to 0.1, which is a smoothing parameter that helps handle cases where some words may not appear in the training data. \n",
    "\n",
    "One instance is named classifier_count, which suggests it will be used with the word count data obtained from the CountVectorizer representation. The other instance is named classifier_tfidf, indicating its use with the TF-IDF vectorized data obtained from the TfidfVectorizer. These classifiers are prepared to be trained and used for text classification purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classfier_count = MultinomialNB(alpha=0.1)\n",
    "classfier_tfidf = MultinomialNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will proceed to train the classifiers with the given inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer Classifier\n",
    "classfier_count.fit(X_train_count,Y_train_count)\n",
    "Y_pred_count = classfier_count.predict(X_test_count)\n",
    "\n",
    "# TFIDF Classifier\n",
    "classfier_tfidf.fit(X_train_tfidf,Y_train_tfidf)\n",
    "Y_pred_tfidf = classfier_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the Passive Aggresive algorithm with TF-IDF or CountVectorizer to allow the model to adapt to new patterns and features that might emerge in the evolving landscape of fake news. This adaptability, coupled with the efficiency of online learning, makes it a powerful tool for addressing the dynamic nature of fake news detection tasks.\n",
    "\n",
    "In the context of fake news detection, the Passive Aggressive algorithm is used in this context for text classification tasks using both the TF-IDF Vectorizer and the CountVectorizer. The algorithm is a popular choice for online learning scenarios where data arrives sequentially and needs to be processed efficiently. It is particularly suitable for binary classification tasks like detecting fake news, spam emails, sentiment analysis, etc.\n",
    "\n",
    "The key concept behind the Passive Aggressive algorithm is that it makes minimal adjustments to its model to correctly classify the current example while aiming to maintain its accuracy on previously seen examples. In other words, it aggressively adjusts its model parameters when a misclassification occurs but remains passive when a correct classification is achieved. This way, it adapts to changes in the data distribution while focusing on minimizing the mistakes on newly arrived data.\n",
    "\n",
    "When combined with TF-IDF or CountVectorizer, the Passive Aggressive algorithm becomes an effective choice for text classification tasks. TF-IDF (Term Frequency-Inverse Document Frequency) and CountVectorizer are techniques used to convert textual data into numerical vectors that can be used by machine learning algorithms. They transform text data into numerical representations that capture the importance of words in the document. Since Passive Aggressive relies on frequent updates to the model, the incremental nature of these vectorization techniques makes them well-suited for online learning scenarios.\n",
    "\n",
    "We will train two different types of classifiers using the Passive Aggressive algorithm: 'passive_classifier_count' is trained using the CountVectorizer transformed data 'X_train_count' and corresponding labels 'Y_train_count', while 'passive_classifier_tfidf' is trained using the TF-IDF Vectorizer transformed data 'X_train_tfidf' and labels 'Y_train_tfidf'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "passive_classifier_count_model = PassiveAggressiveClassifier(n_iter_no_change = 50)\n",
    "passive_classifier_count_model.fit(X_train_count,Y_train_count)\n",
    "Y_pred_count_passive = passive_classifier_count_model.predict(X_test_count)\n",
    "\n",
    "passive_classifier_tfidf_model = PassiveAggressiveClassifier(n_iter_no_change = 50)\n",
    "passive_classifier_tfidf_model.fit(X_train_tfidf,Y_train_tfidf)\n",
    "Y_pred_tfidf_passive = passive_classifier_tfidf_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of two different classifiers trained using the Multinomial Naive Bayes algorithm in combination with two different vectorization techniques, CountVectorizer and TF-IDF.\n",
    "\n",
    "For the CountVectorizer approach:\n",
    "\n",
    "- acc_count stores the accuracy score, which indicates the proportion of correctly predicted labels compared to the actual labels in the test set.\n",
    "- classification_count contains a classification report that provides precision, recall, F1-score, and support for each class (in this case, likely fake news and likely real news).\n",
    "- confusion_matrix_count represents a confusion matrix that shows the true positive, true negative, false positive, and false negative counts.\n",
    "\n",
    "Similarly, for the TF-IDF approach:\n",
    "\n",
    "- acc_tfidf stores the accuracy score.\n",
    "- classification_tfidf contains the classification report.\n",
    "- confusion_matrix_tfidf represents the confusion matrix.\n",
    "\n",
    "These metrics collectively help assess the performance of the classifiers in terms of their ability to correctly classify fake and real news instances. The accuracy score gives an overall indication of the model's performance, while the classification report provides more detailed insights into precision, recall, and F1-score for each class. The confusion matrix further breaks down the results to show where the model's predictions match or deviate from the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation fo CountVectorizer\n",
    "acc_count = accuracy_score(Y_test_count,Y_pred_count)\n",
    "classification_count = classification_report(Y_test_count,Y_pred_count)\n",
    "confusion_matrix_count = confusion_matrix(Y_test_count,Y_pred_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR COUNT VECTORIZER: \n",
      "\n",
      " \n",
      " Accuracy :  0.8561793656580386 \n",
      " Classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87      3086\n",
      "           1       0.82      0.86      0.84      2400\n",
      "\n",
      "    accuracy                           0.86      5486\n",
      "   macro avg       0.85      0.86      0.85      5486\n",
      "weighted avg       0.86      0.86      0.86      5486\n",
      " \n",
      " Confusion matrix \n",
      " [[2624  462]\n",
      " [ 327 2073]] \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"FOR COUNT VECTORIZER: \\n\")\n",
    "print(\" \\n Accuracy : \",acc_count,\"\\n\",\"Classification report \\n\",classification_count,\"\\n\",\"Confusion matrix \\n\",confusion_matrix_count,\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for TFIDF\n",
    "acc_tfidf = accuracy_score(Y_test_tfidf,Y_pred_tfidf)\n",
    "classification_tfidf = classification_report(Y_test_tfidf,Y_pred_tfidf)\n",
    "confusion_matrix_tfidf = confusion_matrix(Y_test_tfidf,Y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR TFIDF VECTORIZER: \n",
      "\n",
      " \n",
      " Accuracy :  0.8454247174626321 \n",
      " Classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87      3086\n",
      "           1       0.87      0.77      0.81      2400\n",
      "\n",
      "    accuracy                           0.85      5486\n",
      "   macro avg       0.85      0.84      0.84      5486\n",
      "weighted avg       0.85      0.85      0.84      5486\n",
      " \n",
      " Confusion matrix \n",
      " [[2800  286]\n",
      " [ 562 1838]]\n"
     ]
    }
   ],
   "source": [
    "print(\"FOR TFIDF VECTORIZER: \\n\")\n",
    "print(\" \\n Accuracy : \",acc_tfidf,\"\\n\",\"Classification report \\n\",classification_tfidf,\"\\n\",\"Confusion matrix \\n\",confusion_matrix_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for CountVectorizer and Passive Agressive algorithm\n",
    "acc_count_passive = accuracy_score(Y_test_count,Y_pred_count_passive)\n",
    "classification_count_passive = classification_report(Y_test_count,Y_pred_count_passive)\n",
    "confusion_matrix_count_passive = confusion_matrix(Y_test_count,Y_pred_count_passive)\n",
    "\n",
    "# Evaluation for TFIDF and Passive Agressive algorithm\n",
    "acc_tfidf_passive = accuracy_score(Y_test_tfidf,Y_pred_tfidf_passive)\n",
    "classification_tfidf_passive = classification_report(Y_test_tfidf,Y_pred_tfidf_passive)\n",
    "confusion_matrix_tfidf_passive = confusion_matrix(Y_test_tfidf,Y_pred_tfidf_passive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR COUNT VECTORIZER - Passive Aggressive Model: \n",
      "\n",
      " \n",
      " Accuracy :  0.8288370397375137 \n",
      " Classification report \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86      3086\n",
      "           1       0.90      0.68      0.78      2400\n",
      "\n",
      "    accuracy                           0.83      5486\n",
      "   macro avg       0.85      0.81      0.82      5486\n",
      "weighted avg       0.84      0.83      0.82      5486\n",
      " \n",
      " Confusion matrix \n",
      "\n",
      " [[2914  172]\n",
      " [ 767 1633]] \n",
      "\n",
      "\n",
      "\n",
      "FOR TFIDF VECTORIZER - Passive Aggressive Model: \n",
      "\n",
      " \n",
      " Accuracy :  0.9101348888078746 \n",
      " Classification report \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.92      3086\n",
      "           1       0.85      0.96      0.90      2400\n",
      "\n",
      "    accuracy                           0.91      5486\n",
      "   macro avg       0.91      0.92      0.91      5486\n",
      "weighted avg       0.92      0.91      0.91      5486\n",
      " \n",
      " Confusion matrix \n",
      "\n",
      " [[2694  392]\n",
      " [ 101 2299]]\n"
     ]
    }
   ],
   "source": [
    "print(\"FOR COUNT VECTORIZER - Passive Aggressive Model: \\n\")\n",
    "print(\" \\n Accuracy : \",acc_count_passive,\"\\n\",\"Classification report \\n\\n\",classification_count_passive,\"\\n\",\"Confusion matrix \\n\\n\",confusion_matrix_count_passive,\"\\n\\n\\n\")\n",
    "\n",
    "print(\"FOR TFIDF VECTORIZER - Passive Aggressive Model: \\n\")\n",
    "print(\" \\n Accuracy : \",acc_tfidf_passive,\"\\n\",\"Classification report \\n\\n\",classification_tfidf_passive,\"\\n\",\"Confusion matrix \\n\\n\",confusion_matrix_tfidf_passive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the model (Fine-tunning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the model and apply fine-tunning, two iterations are performed to the Multinomial Naive Bayes classifiers' performance using different vectorization techniques. \n",
    "\n",
    "For each technique, a range of alpha values is explored to determine the optimal smoothing factor. The code trains and evaluates sub-classifiers with varying alpha values using the training and test datasets. \n",
    "\n",
    "The accuracy scores of these sub-classifiers are calculated and compared, and the best-performing sub-classifiers are selected based on the highest accuracy achieved. Finally, the code prints the best alpha values chosen for CountVectorizer and TF-IDF vectorizers, which represent the smoothing parameters that yielded the highest accuracy scores for the respective classification methods.\n",
    "\n",
    "At the end of the loops, the script prints the best alpha values selected for both CountVectorizer and TF-IDF vectorizers based on the highest accuracy scores achieved during the iterations. These selected classifiers are the ones that demonstrated the best performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing Alpha for Count Vectorizer \n",
      "\n",
      "Alpha : 0.0,Score 0.8563616478308421\n",
      "Alpha : 0.1,Score 0.8561793656580386\n",
      "Alpha : 0.2,Score 0.8559970834852352\n",
      "Alpha : 0.30000000000000004,Score 0.8559970834852352\n",
      "Alpha : 0.4,Score 0.8563616478308421\n",
      "Alpha : 0.5,Score 0.8561793656580386\n",
      "Alpha : 0.6000000000000001,Score 0.8559970834852352\n",
      "Alpha : 0.7000000000000001,Score 0.8554502369668247\n",
      "Alpha : 0.8,Score 0.8554502369668247\n",
      "Alpha : 0.9,Score 0.8552679547940212\n",
      "\n",
      " Choosing Alpha for Tfidf Vectorizer \n",
      "\n",
      "Alpha : 0.0,Score 0.8457892818082392\n",
      "Alpha : 0.1,Score 0.8454247174626321\n",
      "Alpha : 0.2,Score 0.8450601531170252\n",
      "Alpha : 0.30000000000000004,Score 0.8448778709442216\n",
      "Alpha : 0.4,Score 0.8443310244258112\n",
      "Alpha : 0.5,Score 0.8443310244258112\n",
      "Alpha : 0.6000000000000001,Score 0.8441487422530076\n",
      "Alpha : 0.7000000000000001,Score 0.8437841779074007\n",
      "Alpha : 0.8,Score 0.8436018957345972\n",
      "Alpha : 0.9,Score 0.8437841779074007\n",
      "\n",
      "\n",
      " Best Alpha value for count vectorizer: MultinomialNB(alpha=0.0)\n",
      "\n",
      " Best Alpha value for tfidf vectorizer: MultinomialNB(alpha=0.0)\n"
     ]
    }
   ],
   "source": [
    "highest_score_count = 0\n",
    "highest_score_tfidf = 0\n",
    "\n",
    "print(\"Choosing Alpha for Count Vectorizer \\n\")\n",
    "for alpha in np.arange(0,1,0.1):\n",
    "    sub_classifier_count = MultinomialNB(alpha=alpha)\n",
    "    sub_classifier_count.fit(X_train_count,Y_train_count)\n",
    "    Y_pred_hyper_count = sub_classifier_count.predict(X_test_count)\n",
    "    score = accuracy_score(Y_test_count,Y_pred_hyper_count)\n",
    "    if score > highest_score_count:\n",
    "        highest_score_count = score\n",
    "        classfier_count = sub_classifier_count\n",
    "    print(\"Alpha : {},Score {}\".format(alpha,score))\n",
    "\n",
    "print(\"\\n Choosing Alpha for Tfidf Vectorizer \\n\")\n",
    "for alpha in np.arange(0,1,0.1):\n",
    "    sub_classifier_tfidf = MultinomialNB(alpha=alpha)\n",
    "    sub_classifier_tfidf.fit(X_train_tfidf,Y_train_tfidf)\n",
    "    Y_pred_hyper_tfidf = sub_classifier_tfidf.predict(X_test_tfidf)\n",
    "    score = accuracy_score(Y_test_tfidf,Y_pred_hyper_tfidf)\n",
    "    if score > highest_score_tfidf:\n",
    "        highest_score_tfidf = score\n",
    "        classfier_tfidf = sub_classifier_tfidf\n",
    "    print(\"Alpha : {},Score {}\".format(alpha,score))\n",
    "\n",
    "print(\"\\n\\n Best Alpha value for count vectorizer:\",classfier_count)\n",
    "print(\"\\n Best Alpha value for tfidf vectorizer:\" ,classfier_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lists store the vocabulary of words extracted from the text data using the CountVectorizer and TF-IDF vectorizer techniques, respectively. These vocabulary lists represent the unique words present in the corpus after preprocessing and vectorization. The get_feature_names() method is used to retrieve these vocabulary lists from the corresponding vectorizers, allowing access to the terms that were used as features during the transformation of the text data into numerical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_count = count_vectorizer_handler.get_feature_names_out()\n",
    "feature_names_tfidf = tfidf_handler.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize and show the results of the classifiers, we will show the most indicative words associated with fake news classification using both the Count Vectorizer and TF-IDF Vectorizer models. \n",
    "\n",
    "For the Count Vectorizer, it iterates through the sorted list of coefficients and corresponding feature names from the fitted Multinomial Naive Bayes classifier (sub_classifier_count.coef_[0]) and prints the top 10 words that have the highest coefficients (indicative of fake news) in the feature space. \n",
    "\n",
    "Similarly, for the TF-IDF Vectorizer, it iterates through the sorted list of coefficients and corresponding feature names from the fitted Multinomial Naive Bayes classifier (sub_classifier_tfidf.coef_[0]) and prints the top 10 words that have the highest coefficients (indicative of fake news) based on their TF-IDF scores. \n",
    "\n",
    "This allows for the identification of words that contribute most to the model's prediction of fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fake words from Count Vectorizer \n",
      "\n",
      "(-10.267746820534919, 'novemb') \n",
      "\n",
      "(-10.267746820534919, 'stand rock') \n",
      "\n",
      "(-9.320365501590732, 'expos') \n",
      "\n",
      "(-9.320365501590732, 'rig') \n",
      "\n",
      "(-9.320365501590732, 'world war') \n",
      "\n",
      "(-9.13464835579564, 'clinton campaign') \n",
      "\n",
      "(-9.13464835579564, 'dakota') \n",
      "\n",
      "(-8.9780792951041, 'inform') \n",
      "\n",
      "(-8.842737947234339, 'access') \n",
      "\n",
      "(-8.842737947234339, 'podesta') \n",
      "\n",
      "\n",
      " Fake words from Tfidf Vectorizer \n",
      "\n",
      "(-9.548055415839764, 'stand rock') \n",
      "\n",
      "(-9.416570771287159, 'novemb') \n",
      "\n",
      "(-8.823639236206152, 'expos') \n",
      "\n",
      "(-8.805752891290233, 'world war') \n",
      "\n",
      "(-8.702725816409345, 'rig') \n",
      "\n",
      "(-8.689809085760762, 'clinton campaign') \n",
      "\n",
      "(-8.66839809109378, 'dakota') \n",
      "\n",
      "(-8.398669585948337, 'access') \n",
      "\n",
      "(-8.392945915913506, 'podesta') \n",
      "\n",
      "(-8.378189883901959, 'inform') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print and show the most common words in Fake news\n",
    "print(\"\\n Fake words from Count Vectorizer \\n\")\n",
    "for i in (sorted(zip(sub_classifier_count.feature_log_prob_[0],feature_names_count))[:10]):\n",
    "    print(i,\"\\n\")\n",
    "\n",
    "print(\"\\n Fake words from Tfidf Vectorizer \\n\")\n",
    "for i in (sorted(zip(sub_classifier_tfidf.feature_log_prob_[0],feature_names_tfidf))[:10]):\n",
    "    print(i,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both vectorizers, the output lists the top 10 words along with their corresponding coefficient values. In this context, the coefficient values reflect the degree to which each word contributes to the model's prediction of fake news. The words with the lowest coefficients are displayed first, indicating that they are more strongly associated with non-fake news content, while the words with higher coefficients are more indicative of fake news. The output showcases these top words along with their respective coefficients for both vectorizers, allowing for a better understanding of the words that play a significant role in differentiating fake news from non-fake news content based on their frequency and importance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same happens for real news, the most common words are shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Real words from Count Vectorizer \n",
      "\n",
      "(-2.4626368651881982, 'new') \n",
      "\n",
      "(-2.506943328884356, 'time') \n",
      "\n",
      "(-2.512791430303773, 'york') \n",
      "\n",
      "(-2.5130170417116418, 'new york') \n",
      "\n",
      "(-2.528021074119925, 'york time') \n",
      "\n",
      "(-2.528021074119925, 'new york time') \n",
      "\n",
      "(-3.4677525970208407, 'breitbart') \n",
      "\n",
      "(-3.5262947120503254, 'trump') \n",
      "\n",
      "(-4.836786613903009, 'donald') \n",
      "\n",
      "(-4.841406626740659, 'donald trump') \n",
      "\n",
      "\n",
      " Real words from Tfidf Vectorizer \n",
      "\n",
      "(-2.978812622645214, 'new') \n",
      "\n",
      "(-2.9942520504672308, 'york') \n",
      "\n",
      "(-2.994286679829563, 'new york') \n",
      "\n",
      "(-2.9964846805172494, 'time') \n",
      "\n",
      "(-3.005574117457769, 'york time') \n",
      "\n",
      "(-3.005574117457769, 'new york time') \n",
      "\n",
      "(-3.6047944206732234, 'breitbart') \n",
      "\n",
      "(-3.8603635117038833, 'trump') \n",
      "\n",
      "(-4.879406855825993, 'donald') \n",
      "\n",
      "(-4.881461370584863, 'donald trump') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print and show the most common words in Real news\n",
    "print(\"\\n Real words from Count Vectorizer \\n\")\n",
    "for i in (sorted(zip(sub_classifier_count.feature_log_prob_[0],feature_names_count),reverse=True)[:10]):\n",
    "    print(i,\"\\n\")\n",
    "\n",
    "print(\"\\n Real words from Tfidf Vectorizer \\n\")\n",
    "for i in (sorted(zip(sub_classifier_tfidf.feature_log_prob_[0],feature_names_tfidf),reverse=True)[:10]):\n",
    "    print(i,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, for this dataset, the words \"trump,\" \"hillary,\" \"clinton,\" \"election,\" and other terms related to political figures and events are prominent in both vectorizers, suggesting their importance in real news classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'passive_aggresive_model_count.sav'\n",
    "pickle.dump(passive_classifier_count_model, open(filename, 'wb'))\n",
    "\n",
    "filename = 'passive_aggresive_model_tfidf.sav'\n",
    "pickle.dump(passive_classifier_tfidf_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues, if Not Purse...</td>\n",
       "      <td>David Streitfeld</td>\n",
       "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>#NoDAPL: Native American Leaders Vow to Stay A...</td>\n",
       "      <td>Common Dreams</td>\n",
       "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>Tim Tebow Will Attempt Another Comeback, This ...</td>\n",
       "      <td>Daniel Victor</td>\n",
       "      <td>If at first you don’t succeed, try a different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>Keiser Report: Meme Wars (E995)</td>\n",
       "      <td>Truth Broadcast Network</td>\n",
       "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>25995</td>\n",
       "      <td>The Bangladeshi Traffic Jam That Never Ends - ...</td>\n",
       "      <td>Jody Rosen</td>\n",
       "      <td>Of all the dysfunctions that plague the world’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>25996</td>\n",
       "      <td>John Kasich Signs One Abortion Bill in Ohio bu...</td>\n",
       "      <td>Sheryl Gay Stolberg</td>\n",
       "      <td>WASHINGTON  —   Gov. John Kasich of Ohio on Tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>25997</td>\n",
       "      <td>California Today: What, Exactly, Is in Your Su...</td>\n",
       "      <td>Mike McPhate</td>\n",
       "      <td>Good morning. (Want to get California Today by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>25998</td>\n",
       "      <td>300 US Marines To Be Deployed To Russian Borde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>« Previous - Next » 300 US Marines To Be Deplo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>25999</td>\n",
       "      <td>Awkward Sex, Onscreen and Off - The New York T...</td>\n",
       "      <td>Teddy Wayne</td>\n",
       "      <td>Perhaps you’ve seen the new TV series whose pi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0     20800  Specter of Trump Loosens Tongues, if Not Purse...   \n",
       "1     20801  Russian warships ready to strike terrorists ne...   \n",
       "2     20802  #NoDAPL: Native American Leaders Vow to Stay A...   \n",
       "3     20803  Tim Tebow Will Attempt Another Comeback, This ...   \n",
       "4     20804                    Keiser Report: Meme Wars (E995)   \n",
       "...     ...                                                ...   \n",
       "5195  25995  The Bangladeshi Traffic Jam That Never Ends - ...   \n",
       "5196  25996  John Kasich Signs One Abortion Bill in Ohio bu...   \n",
       "5197  25997  California Today: What, Exactly, Is in Your Su...   \n",
       "5198  25998  300 US Marines To Be Deployed To Russian Borde...   \n",
       "5199  25999  Awkward Sex, Onscreen and Off - The New York T...   \n",
       "\n",
       "                       author  \\\n",
       "0            David Streitfeld   \n",
       "1                         NaN   \n",
       "2               Common Dreams   \n",
       "3               Daniel Victor   \n",
       "4     Truth Broadcast Network   \n",
       "...                       ...   \n",
       "5195               Jody Rosen   \n",
       "5196      Sheryl Gay Stolberg   \n",
       "5197             Mike McPhate   \n",
       "5198                      NaN   \n",
       "5199              Teddy Wayne   \n",
       "\n",
       "                                                   text  \n",
       "0     PALO ALTO, Calif.  —   After years of scorning...  \n",
       "1     Russian warships ready to strike terrorists ne...  \n",
       "2     Videos #NoDAPL: Native American Leaders Vow to...  \n",
       "3     If at first you don’t succeed, try a different...  \n",
       "4     42 mins ago 1 Views 0 Comments 0 Likes 'For th...  \n",
       "...                                                 ...  \n",
       "5195  Of all the dysfunctions that plague the world’...  \n",
       "5196  WASHINGTON  —   Gov. John Kasich of Ohio on Tu...  \n",
       "5197  Good morning. (Want to get California Today by...  \n",
       "5198  « Previous - Next » 300 US Marines To Be Deplo...  \n",
       "5199  Perhaps you’ve seen the new TV series whose pi...  \n",
       "\n",
       "[5200 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the CountVectorizer and TFIDF classifiers with MultinominalNB, we must prepare the testing data. \n",
    "\n",
    "First of all, we will preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the sum of missing values for each column in the dataset in order to identify how many missing values are present in each column, providing insight into the completeness of the data and allowing for potential data handling strategies, such as imputation or removal of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "title     122\n",
       "author    503\n",
       "text        7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code segment prepares the text data in the 'corpus_test' by using two different vectorization techniques: CountVectorizer and TF-IDF Vectorizer. For CountVectorizer, the CountVectorizer class from the scikit-learn library is used, with the maximum number of features set to 'MAX_FEATURES_VECTORIZER' and an n-gram range of (1,3). The text data is then transformed into a numerical matrix using the fit_transform() function and stored in 'X_test_count'. Similarly, for TF-IDF Vectorizer, the TfidfVectorizer class is used with the same parameters, and the transformed matrix is stored in 'X_test_tfidf'. These matrix representations of the text data are used for testing the trained machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare vectorizers (CountVectorizer)\n",
    "count_vectorizer_handler_test = CountVectorizer(max_features=MAX_FEATURES_VECTORIZER,ngram_range=(1,3))\n",
    "X_test_count = count_vectorizer_handler_test.fit_transform(corpus_test).toarray()\n",
    "\n",
    "# Prepare vectorizers (TFIDFVectorizer)\n",
    "tfidf_handler_test = TfidfVectorizer(max_features=MAX_FEATURES_VECTORIZER,ngram_range=(1,3))\n",
    "X_test_tfidf = tfidf_handler_test.fit_transform(corpus_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_count_passive = passive_classifier_count_model.predict(X_test_count)\n",
    "Y_pred_tfidf_passive = passive_classifier_tfidf_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues, if Not Purse...</td>\n",
       "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>#NoDAPL: Native American Leaders Vow to Stay A...</td>\n",
       "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>Tim Tebow Will Attempt Another Comeback, This ...</td>\n",
       "      <td>If at first you don’t succeed, try a different...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>Keiser Report: Meme Wars (E995)</td>\n",
       "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  20800  Specter of Trump Loosens Tongues, if Not Purse...   \n",
       "1  20801  Russian warships ready to strike terrorists ne...   \n",
       "2  20802  #NoDAPL: Native American Leaders Vow to Stay A...   \n",
       "3  20803  Tim Tebow Will Attempt Another Comeback, This ...   \n",
       "4  20804                    Keiser Report: Meme Wars (E995)   \n",
       "\n",
       "                                                text  label  \n",
       "0  PALO ALTO, Calif.  —   After years of scorning...      0  \n",
       "1  Russian warships ready to strike terrorists ne...      0  \n",
       "2  Videos #NoDAPL: Native American Leaders Vow to...      1  \n",
       "3  If at first you don’t succeed, try a different...      1  \n",
       "4  42 mins ago 1 Views 0 Comments 0 Likes 'For th...      1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions Count Vectorizer and Passive Aggresive ALgorithm\n",
    "submission= pd.DataFrame({\"id\":test_data.id, \"title\": test_data.title, \"text\": test_data.text, \"label\":Y_pred_count_passive})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues, if Not Purse...</td>\n",
       "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>#NoDAPL: Native American Leaders Vow to Stay A...</td>\n",
       "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>Tim Tebow Will Attempt Another Comeback, This ...</td>\n",
       "      <td>If at first you don’t succeed, try a different...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>Keiser Report: Meme Wars (E995)</td>\n",
       "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  20800  Specter of Trump Loosens Tongues, if Not Purse...   \n",
       "1  20801  Russian warships ready to strike terrorists ne...   \n",
       "2  20802  #NoDAPL: Native American Leaders Vow to Stay A...   \n",
       "3  20803  Tim Tebow Will Attempt Another Comeback, This ...   \n",
       "4  20804                    Keiser Report: Meme Wars (E995)   \n",
       "\n",
       "                                                text  label  \n",
       "0  PALO ALTO, Calif.  —   After years of scorning...      0  \n",
       "1  Russian warships ready to strike terrorists ne...      1  \n",
       "2  Videos #NoDAPL: Native American Leaders Vow to...      1  \n",
       "3  If at first you don’t succeed, try a different...      1  \n",
       "4  42 mins ago 1 Views 0 Comments 0 Likes 'For th...      1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions TFIDF and Passive Aggresive ALgorithm\n",
    "submission= pd.DataFrame({\"id\":test_data.id, \"title\": test_data.title, \"text\": test_data.text, \"label\":Y_pred_tfidf_passive})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code demonstrates the use of two prominent text vectorization techniques, TF-IDF and CountVectorizer, along with the Passive Aggressive algorithm for fake news detection. TF-IDF and CountVectorizer are powerful methods for converting textual data into numerical representations, capturing the importance of words within the document. In the context of fake news detection, these techniques play a pivotal role in identifying key terms and features that can help distinguish between reliable and unreliable news articles.\n",
    "\n",
    "The Passive Aggressive algorithm is employed due to its suitability for online learning, which is advantageous when dealing with a dynamic domain like fake news detection. It incrementally updates the model to adapt to changes in the data distribution while focusing on maintaining accuracy on previously seen examples. This adaptability is crucial for tracking evolving patterns of fake news.\n",
    "\n",
    "In the context of a dataset focused on the 2017 political campaign for the presidency of Donald Trump and Hillary Clinton, the combination of TF-IDF, CountVectorizer, and the Passive Aggressive algorithm becomes particularly relevant. News articles related to political campaigns often involve complex language patterns and sentiment, which can be captured by these vectorization techniques. Identifying key terms and phrases specific to the campaign, candidates, or controversial topics is essential for detecting fake news.\n",
    "\n",
    "Through the TF-IDF and CountVectorizer techniques, the model can automatically extract and assign weights to terms that are highly relevant to either reliable or unreliable news articles. By using the Passive Aggressive algorithm, the model continuously learns from incoming data, adjusting its parameters to minimize mistakes and adapt to emerging patterns. This approach enables the model to effectively differentiate between reliable and unreliable news articles, especially in the context of a dynamic political campaign.\n",
    "\n",
    "Overall, the combination of TF-IDF, CountVectorizer, and the Passive Aggressive algorithm serves as a robust approach to fake news detection, particularly for datasets involving political campaigns. By leveraging the importance of specific terms, the model can provide insights into the linguistic characteristics associated with reliable and unreliable news, contributing to more accurate and timely identification of fake news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF and CountVectorizer are relatively simple and interpretable methods, making them easy to implement and understand. They work well when the focus is on feature extraction and capturing the importance of words in a document. These techniques are particularly useful when dealing with smaller datasets, where complex Deep Learning models might overfit.\n",
    "\n",
    "On the other hand, Deep Learning models, such as Recurrent Neural Networks (RNNs) and Transformers, have the ability to capture intricate relationships in text data, including contextual nuances and long-range dependencies. They excel in handling vast amounts of text data and can automatically learn complex patterns and representations. However, Deep Learning models often require more computational resources and larger datasets to achieve optimal performance. They also tend to be less interpretable, making it challenging to understand why specific predictions are made.\n",
    "\n",
    "In the realm of fake news detection, both approaches have their merits. TF-IDF and CountVectorizer are advantageous when computational resources are limited and when quick insights are required. They are useful for tasks where feature extraction and identifying key terms are the main goals. On the other hand, Deep Learning models shine when dealing with massive text datasets, capturing subtle linguistic nuances, and adapting to evolving patterns in news articles.\n",
    "\n",
    "Furthermore, combining both approaches can lead to hybrid models that harness the strengths of both traditional NLP techniques and Deep Learning. For example, using TF-IDF or CountVectorizer as input features for a Deep Learning model can enhance the model's performance by providing it with rich contextual information.\n",
    "\n",
    "In the context of fake news detection in the given dataset, using TF-IDF, CountVectorizer, and the Passive Aggressive algorithm provides a solid foundation. However, exploring Deep Learning models, such as LSTM or BERT, could also yield valuable insights, especially in scenarios where the data is vast, complex, and requires more nuanced understanding of text semantics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
