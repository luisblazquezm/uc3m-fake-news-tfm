{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAKE NEWS DETECTOR - TRANSFORMERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLAN OF ACTION\n",
    "\n",
    "- Load the dataset, i.e. combine it into one, and create true / false labels\n",
    "- Then, utilizing HuggingFace's Transformers library, retrieve the pre-trained BERT model to use as the foundation of a Fake News Detection Model. BERT has incredible language understanding abilities. As a result, it will help the model better grasp news context and hence generate educated predictions about whether news is false or not - The Base Model and general architecture are then defined. For example, we may use PyTorch to define, train, and evaluate neural network models.\n",
    "- After that, we freeze the weights on the BERT beginning layers. If we do not accomplish this, we will lose all of our past knowledge.\n",
    "- Next, we add new Trainable Layers. In general, feature extraction layers are the only information from the basic model that we utilize. We must add additional layers on top of the model to forecast its particular jobs. Furthermore, we establish a new output layer because the pre-trained model's ultimate output will almost definitely differ from the result we desire for our model, which is binary 0 and 1.\n",
    "- The final stage is to fine-tune our model. Making minor tweaks to the model to improve it.\n",
    "- After that, we use our Fake News Detection Model on unseen data to create predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install pycaret\n",
    "\n",
    "#### Scikit-learn\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#### Transformers\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from transformers import AdamW\n",
    "\n",
    "#### Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#### Others\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycaret\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH = \"./data/bert/en/\"\n",
    "TRAIN_RATIO = 0.7\n",
    "TEST_RATIO = 0.3\n",
    "PYTORCH_BATCHS_SIZE = 32\n",
    "TRAINING_EPOCHS = 2 # Number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------- FROM HERE USE COLLAB (WITH GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are on Colab, activate the GPU runtime by clicking on \n",
    "\n",
    "Runtime -> Change runtime type -> Select GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we are on Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:  # Set Working Directory - if working on Local Machine\n",
    "    import os\n",
    "    os.chdir('/Users//replace_me')\n",
    "else: # Set Working Directory - if working on Google Drive\n",
    "    # Mount Google Drive - applicable, if working on Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/1_LiveProjects/Project11_FakeNewsDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset chosen comes from Kaggle: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset. \n",
    "\n",
    "It has two separate .csv files, one having the real news, called \"True.csv\" and another one, having the fake news, called \"Fake.csv\". Both files have the exact same data form.\n",
    "\n",
    "They both include the title of the news, the entire news article text, the subject, which is basically the category of news and the date on which it was published. \n",
    "\n",
    "Here, we install Huggingface’s transformers library, which allows us to import a wide range of transformer-based pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "true_data = pd.read_csv(DATA_BASE_PATH + 'True.csv')\n",
    "fake_data = pd.read_csv(DATA_BASE_PATH + 'Fake.csv')\n",
    "\n",
    "# Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'\n",
    "true_data['Target'] = ['True']*len(true_data)\n",
    "fake_data['Target'] = ['Fake']*len(fake_data)\n",
    "\n",
    "# Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'\n",
    "data = true_data.append(fake_data).sample(frac=1).reset_index().drop(columns=['index'])\n",
    "\n",
    "# Target column is made of string values True/Fake, let's change it to numbers 0/1 (Fake=1) \n",
    "data['label'] = pd.get_dummies(data.Target)['Fake']\n",
    "\n",
    "# Checking if our data is well balanced\n",
    "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
    "plt.pie(label_size,explode=[0.1,0.1],colors=['firebrick','navy'],startangle=90,shadow=True,labels=['Fake','True'],autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import genuine and fake csv files into a pandas dataframe. Then, in the 'Target' column, we insert the labels True / Fake. Finally, we use random mixing to combine the two dataframes into a single dataframe.\n",
    "\n",
    "Following that, the target column contains string values that a machine cannot interpret. As a result, we must convert them to numeric representation. To do this, we utilize Pandas get_dummies to construct a new column named label, in which we assign 1 to all Fake labels and 0 to all True labels. We may use a pie chart at the end to see if our data is balanced across the two labels. As you can see, our data is rather balanced.\n",
    "\n",
    "Following that, we divided our data into 70 % trainning and 30 % validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation-Test set split into 70:15:15 ratio\n",
    "\n",
    "# Train-Temp split (70:30 -> 0.3 OF 100 TEST)\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(data['title'], data['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=TEST_RATIO, \n",
    "                                                                    stratify=data['Target'])\n",
    "# Validation-Test split (15:15 -> 0.5 OF 30 TEST)\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this use case, these files shall be merged into a single large dataset, and add a new column ‘Label’, that will have ‘true’ mentioned against all observations from the true.csv and ‘fake’ mentioned against the observations from fake.csv\n",
    "\n",
    "Now we come to the BERT fine-tuning stage, where we shall perform transfer learning.\n",
    "\n",
    "The steps are the following:\n",
    "\n",
    "- First up, we import BERT-base model that has 110 million parameters. Along with it, we also import the BERT Tokenizer\n",
    "- We preprocess our input data. We shall use the title of the news article to train our model.\n",
    "- Now, let’s figure out how to standardize the word length of our news titles, as it would vary from one article title to another. For this, we plot a word count histogram, to understand what the typical word length is like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer via HuggingFace Transformers\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Plot histogram of the number of words in train data 'title'\n",
    "seq_len = [len(title.split()) for title in train_text]\n",
    "pd.Series(seq_len).hist(bins = 40,color='firebrick')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Number of texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### >>>>> NOTE\n",
    "\n",
    "We can see the majority of titles have word length under 15 words. So, we shall perform padding on all our titles to limit them to 15 words in the tokenization stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we transform integer sequences to tensors. Finally, data loaders for both the train and validation sets are defined. During the training phase, these data loaders will send batches of train and validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of titles above have word length under 15. So, we set max title length as 15\n",
    "max_length_content = 15\n",
    "\n",
    "# Tokenize and encode sequences in the train set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_length_content,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = max_length_content,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_length_content,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Convert lists to tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "# Data Loader structure definition\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)    # wrap tensors\n",
    "train_sampler = RandomSampler(train_data)                     # sampler for sampling the data during training\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=PYTORCH_BATCHS_SIZE)\n",
    "                                                              # dataLoader for train set\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)            # wrap tensors\n",
    "val_sampler = SequentialSampler(val_data)                     # sampler for sampling the data during training\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=PYTORCH_BATCHS_SIZE)\n",
    "                                                              # dataLoader for validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we will freeze pre-trained model weights before fine-tuning it.This will prevent model weights from being updated during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing the parameters and defining trainable BERT structure\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False    # false here means gradient need not be computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be the definition of the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):  \n",
    "      super(BERT_Arch, self).__init__()\n",
    "      self.bert = bert   \n",
    "      self.dropout = nn.Dropout(0.1)            # dropout layer\n",
    "      self.relu =  nn.ReLU()                    # relu activation function\n",
    "      self.fc1 = nn.Linear(768,512)             # dense layer 1\n",
    "      self.fc2 = nn.Linear(512,2)               # dense layer 2 (Output layer)\n",
    "      self.softmax = nn.LogSoftmax(dim=1)       # softmax activation function\n",
    "\n",
    "    def forward(self, sent_id, mask):           # define the forward pass  \n",
    "      cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']\n",
    "                                                # pass the inputs to the model\n",
    "      x = self.fc1(cls_hs)\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc2(x)                           # output layer\n",
    "      x = self.softmax(x)                       # apply softmax activation\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_Arch(bert)\n",
    "\n",
    "# Defining the hyperparameters (optimizer, weights of the classes and the epochs)\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5)          # learning rate\n",
    "\n",
    "# Define the loss function\n",
    "cross_entropy  = nn.NLLLoss() TRAINING_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using PyTorch for defining, training, & evaluating our deep learning model. Post our BERT network, we are adding dense layers 1 & 2 followed by softmax activation. Then, we define our hyperparameters; we are using AdamW as our optimizer.\n",
    "\n",
    "#### >>>>> NOTE\n",
    "\n",
    "Then we define our loss function. And lastly, we are keeping number of epochs to 2. With Colab’s free GPU, one epoch might take up to 20 mins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is a large neural network design with a large number of parameters ranging from 100 million to more than 300 million. Overfitting might occur if a BERT model was trained from scratch on a tiny dataset. As a result, it is preferable to begin with a pre-trained BERT model that was trained on a large dataset. We may then train the model again on our reduced dataset, a technique known as model fine-tuning. There are three ways that may be used to accomplish this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first is to train the entire architecture (the pre-trained weights of the entire model are updated on the new dataset), after which we may train the complete pre-trained model on our dataset and feed the result to a softmax layer. In this situation, the mistake is propagated throughout the architecture, and the model's pre-trained weights are adjusted depending on the new dataset.\n",
    "- The second method is to train certain layers while freezing others (by freezing the weights of the beginning layers while retraining just the upper layers using tria-&-test), which is equivalent to partly training a pre-trained model. We maintain the weights of the model's early layers fixed while retraining just the upper levels. We may use trial and error to determine how many layers to use.\n",
    "- And the third one, is to Freeze the entire architecture (and attach a few neural network layers of our own and train the new model), where we basically freeze all the layers of the pre-trained model and attach a few neural network layers of our own and train this new model. Note that the weights of only the attached layers will be updated during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining training function\n",
    "def train():  \n",
    "  model.train()\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  for step,batch in enumerate(train_dataloader):                # iterate over batches\n",
    "    if step % 50 == 0 and not step == 0:                        # progress update after every 50 batches.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "    \n",
    "    batch = [r for r in batch]                                  # push the batch to gpu\n",
    "    sent_id, mask, labels = batch \n",
    "    model.zero_grad()                                           # clear previously calculated gradients\n",
    "    preds = model(sent_id, mask)                                # get model predictions for current batch\n",
    "    loss = cross_entropy(preds, labels)                         # compute loss between actual & predicted values\n",
    "    total_loss = total_loss + loss.item()                       # add on to the total loss\n",
    "    loss.backward()                                             # backward pass to calculate the gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)     # clip gradients to 1.0. It helps in preventing exploding gradient problem\n",
    "    optimizer.step()                                            # update parameters\n",
    "    preds=preds.detach().cpu().numpy()                          # model predictions are stored on GPU. So, push it to CPU\n",
    "\n",
    "  avg_loss = total_loss / len(train_dataloader)                 # compute training loss of the epoch  \n",
    "                                                                # reshape predictions in form of (# samples, # classes)\n",
    "  return avg_loss                                 # returns the loss and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():  \n",
    "  print(\"\\nEvaluating...\")  \n",
    "  model.eval()                                    # Deactivate dropout layers\n",
    "  total_loss, total_accuracy = 0, 0  \n",
    "  for step,batch in enumerate(val_dataloader):    # Iterate over batches  \n",
    "    if step % 50 == 0 and not step == 0:          # Progress update every 50 batches.     \n",
    "                                                  # Calculate elapsed time in minutes.\n",
    "                                                  # Elapsed = format_time(time.time() - t0)\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "                                                  # Report progress\n",
    "    batch = [t for t in batch]                    # Push the batch to GPU\n",
    "    sent_id, mask, labels = batch\n",
    "    with torch.no_grad():                         # Deactivate autograd\n",
    "      preds = model(sent_id, mask)                # Model predictions\n",
    "      loss = cross_entropy(preds,labels)          # Compute the validation loss between actual and predicted values\n",
    "      total_loss = total_loss + loss.item()\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "  avg_loss = total_loss / len(val_dataloader)         # compute the validation loss of the epoch\n",
    "  return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and predict\n",
    "best_valid_loss = float('inf')\n",
    "train_losses=[]                   # empty lists to store training and validation loss of each epoch\n",
    "valid_losses=[]\n",
    "\n",
    "for epoch in range(epochs):     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))     \n",
    "    train_loss = train()                       # train model\n",
    "    valid_loss = evaluate()                    # evaluate model\n",
    "    if valid_loss < best_valid_loss:              # save the best model\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'c2_new_model_weights.pt')\n",
    "    train_losses.append(train_loss)               # append training and validation loss\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classification report on the test set using our fake news model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights of best model\n",
    "path = 'c1_fakenews_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq, test_mask)\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "  \n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try runnning predictions on these sample news titles. First two are fake and the next two are real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on unseen data\n",
    "unseen_news_text = [\n",
    "    \"Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing\",     # Fake\n",
    "    \"WATCH: George W. Bush Calls Out Trump For Supporting White Supremacy\",               # Fake\n",
    "    \"U.S. lawmakers question businessman at 2016 Trump Tower meeting: sources\",           # True\n",
    "    \"Trump administration issues new rules on U.S. visa waivers\"                          # True\n",
    "]\n",
    "# tokenize and encode sequences in the test set\n",
    "max_length_content = 15\n",
    "tokens_unseen = tokenizer.batch_encode_plus(\n",
    "    unseen_news_text,\n",
    "    max_length = max_length_content,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "unseen_seq = torch.tensor(tokens_unseen['input_ids'])\n",
    "unseen_mask = torch.tensor(tokens_unseen['attention_mask'])\n",
    "\n",
    "with torch.no_grad():\n",
    "  preds = model(unseen_seq, unseen_mask)\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "preds = np.argmax(preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both precision & recall for class 1 are quite high which means that the model predicts this class pretty well. Looking at the recall for class 1, it is 0.85 which means that the model was able to correctly classify 85% of the fake news as fake. Precision is 0.92, which means that 92% of the fake news classifications by the model, are actually fake news."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
