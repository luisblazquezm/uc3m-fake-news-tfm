{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAKE NEWS DETECTOR - ENSEMBLE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLAN OF ACTION\n",
    "\n",
    "- Load the dataset, i.e. combine it into one, and create true / false labels\n",
    "- Then, utilizing HuggingFace's Transformers library, retrieve the pre-trained BERT model to use as the foundation of a Fake News Detection Model. BERT has incredible language understanding abilities. As a result, it will help the model better grasp news context and hence generate educated predictions about whether news is false or not - The Base Model and general architecture are then defined. For example, we may use PyTorch to define, train, and evaluate neural network models.\n",
    "- After that, we freeze the weights on the BERT beginning layers. If we do not accomplish this, we will lose all of our past knowledge.\n",
    "- Next, we add new Trainable Layers. In general, feature extraction layers are the only information from the basic model that we utilize. We must add additional layers on top of the model to forecast its particular jobs. Furthermore, we establish a new output layer because the pre-trained model's ultimate output will almost definitely differ from the result we desire for our model, which is binary 0 and 1.\n",
    "- The final stage is to fine-tune our model. Making minor tweaks to the model to improve it.\n",
    "- After that, we use our Fake News Detection Model on unseen data to create predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#### Others\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_1_PATH = \"./data/ensemble_learning/dataset1/\"\n",
    "DATA_BASE_2_PATH = \"./data/ensemble_learning/dataset2/\"\n",
    "NUM_RECORDS_TO_SAMPLE = 3000\n",
    "TRAIN_RATIO = 0.7\n",
    "TEST_RATIO = 0.3\n",
    "PYTORCH_BATCHS_SIZE = 32\n",
    "TRAINING_EPOCHS = 2 # Number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset chosen comes from Kaggle: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset. \n",
    "\n",
    "It has two separate .csv files, one having the real news, called \"True.csv\" and another one, having the fake news, called \"Fake.csv\". Both files have the exact same data form.\n",
    "\n",
    "They both include the title of the news, the entire news article text, the subject, which is basically the category of news and the date on which it was published. \n",
    "\n",
    "We must now put them together if we are to conduct training and testing.\n",
    "\n",
    "Let us begin by importing pandas.\n",
    "\n",
    "The two csv files are converted into pandas dataframes and named.\n",
    "\n",
    "A categorization column is required to determine if a record is false news or not. We create a new column named classification and set all of its entries to 1 with the command fake['classification'] = 1. Our classifier will detect bogus news and assign it a value of one.\n",
    "\n",
    "The index counts of the two dataframes are then reset by setting the option ignore_index to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "true_data = pd.read_csv(DATA_BASE_1_PATH + 'True.csv')\n",
    "fake_data = pd.read_csv(DATA_BASE_1_PATH + 'Fake.csv')\n",
    "\n",
    "# Adding classification column\n",
    "fake_data['classification'] = 1\n",
    "true_data['classification'] = 0\n",
    "\n",
    "# Concatenate two dataframes together\n",
    "true_fake = pd.concat([true_data,fake_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of data exploration is to visualize data in order to conduct a more in-depth research, find early insights, and detect trends. We might begin by simply charting the amount of news items in each category:\n",
    "\n",
    "When we import matplotlib, a new variable called category_dist is created that counts the number of times the numbers 1 and 0 appear within the classification column. Using plt and a series of commands, we can select the size, type of chart, color, labels, title, and whether or not we want the gridlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting values within each classification bucket\n",
    "category_dist = true_fake['classification'].value_counts()\n",
    "\n",
    "# Defining chart\n",
    "plt.figure(figsize=(8,6))\n",
    "category_dist.plot(kind='bar', color = '#89b4a1')\n",
    "plt.xlabel(\"Classification\", fontsize = 12)\n",
    "plt.ylabel(\"Number of News Headlines by Classification\", fontsize = 12)\n",
    "plt.title(\"Distribution of News Headlines by Classification\", fontsize = 15)\n",
    "plt.grid(False)\n",
    "\n",
    "# Generating chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe contains around 40.000 records, with somewhat more bogus news. We don't need to use any unbalanced data treatment strategies like upsampling because the difference is so small.\n",
    "\n",
    "Feeding the model selection pipeline with more than 40.000 records, each of which has 750 words or more, may result in a RAM memory overflow. A easy method is to choose a smaller percentage of the records.\n",
    "\n",
    "Pandas provides an easy-to-use command that allows the developer to sample a defined number of records while creating a random_state for repeatability of findings. I chose 3000 records since it is an arbitrary amount that represents slightly less than 10% of the dataframe and should not create RAM overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = true_fake.sample(NUM_RECORDS_TO_SAMPLE, replace=True, random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting values within each classification bucket\n",
    "category_dist_sample = sample_df['classification'].value_counts()\n",
    "\n",
    "# Defining chart\n",
    "plt.figure(figsize=(8,6))\n",
    "category_dist_sample.plot(kind='bar', color = '#89b4a1')\n",
    "plt.xlabel(\"Classification\", fontsize = 12)\n",
    "plt.ylabel(\"Number of News Headlines by Classification\", fontsize = 12)\n",
    "plt.title(\"Distribution of News Headlines by Classification\", fontsize = 15)\n",
    "plt.grid(False)\n",
    "\n",
    "# Generating chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with 3000 records, the distribution between fake and truthful news stays the same. This means we can move on to the next phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another dataset only for testing purposes will be used to deploy the model on unseen data. Records were scraped from 244 websites based on the categorization of a Chrome Extension called BS Detector. You can find the dataset at the following link (https://www.kaggle.com/datasets/mrisdal/fake-news). The major difference is that this one contains only fake reviews. It has 21 columns in total, among which the most relevant are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import genuine and fake csv files into a pandas dataframe. Then, in the 'Target' column, we insert the labels True / Fake. Finally, we use random mixing to combine the two dataframes into a single dataframe.\n",
    "\n",
    "For the classification problem, for the three datasets, we only need text and information on whether the news is truthful or not correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleansing is essential in NLP. Stopwords, special characters, and HTML elements all add unnecessary complexity to the processing of our models.\n",
    "\n",
    "Eliminating all of the above may result in a reduction in training and testing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate (Model Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the project, I considered 7 common classification models, including parametric and non-parametric ones. All of them are supervised, learning models:\n",
    "\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- k-Nearest-Neighbours (kNN)\n",
    "- Gaussian Naive Bayes (GaussianNB)\n",
    "- Logistic regression (LR)\n",
    "- Decision tree classifier (CART)\n",
    "- Support Vector Machines (SVM)\n",
    "- Linear SVM\n",
    "- Random Forest\n",
    "- GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to put the model selection process into action. The code runs data through a count vectorizer and all of the previously mentioned models before doing k-fold cross-validation. After that, the average accuracy is determined and displayed in a boxplot chart. Let's take it one step at a time:\n",
    "\n",
    "\n",
    "- scikit-learn and matplotlib are these the only two libraries required? Sklearn contains all of the functions required for doing machine learning on data. Every model is imported using a \"sub-library\": LogisticRegression, KNeighborsClassifier, DecisionTreeClassifier, SVC, GaussianNB, and LinearDiscriminantAnalysis are all examples of classification algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At this point, we may specify the input and target variables based on the data. The input variable xis \"text,\" which contains the review corpus; the output variable yis \"classification,\" which displays the labels 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining input and target variable\n",
    "x_input = sample_df['text']\n",
    "y_target = sample_df['classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We know that some of the models under consideration, such as decision tree classifiers, require a dense matrix. Dense matrices have a high proportion of non-zero values. To avoid errors, the ToDenseTransformerclass ensures that all matrices are dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding To Dense Transfomer to satisfy dense data requirement\n",
    "class ToDenseTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X.todense()\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The list models is then constructed, and each model's object is appended to the list. The list results, on the other hand, will include all of the distinct model scores linked with their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(max_iter=10000)))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The k-fold option specifies the number of k-folds desired. The notion of cross-validation is introduced here. Cross-validation tries to improve model accuracy estimation. It is known as k-fold cross validation, where k is the number of sub-groups into which the data is separated. The model is trained on a subset of the data and then tested on the remainder k-1. The accuracy is calculated on the average score.\n",
    "\n",
    "- The pipeline runs cross-validation on the k-fold of choice and applies the count vectorizer, dense transformer, and model of choice. The findings are then printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "\n",
    "#Building pipeline and gathering results\n",
    "for name, model in models:\n",
    "    classifier = Pipeline([('vect', CountVectorizer(strip_accents=None, lowercase = False, preprocessor = None)), ('todense', ToDenseTransformer()), (name, model)])\n",
    "    cv_results = model_selection.cross_val_score(classifier, x_input, y_target, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: Average accuarcy = %f (Variance = %f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, matplotlib creates a boxplot chart to help us analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A boxplot provides information on five crucial accuracy statistics for our models:\n",
    "\n",
    "- the bare minimum\n",
    "- first quartile (25th percentile)\n",
    "- the average (second quartile)\n",
    "- third quartile (75th percentile)\n",
    "- the very best.\n",
    "\n",
    "All of the models used for cross-validation are plotted on the x-axis. Instead, we have the accuracy score on the y-axis. From the chart alone, it’s hard to tell exactly how well each model performed; for example, the logistic regression and the decision tree classifier are very close to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>>>>>>>>>>>>>>> NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART is the best performer in terms of accuracy but the logistic regression is slightly better (by a very thin margin) at delivering consistent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving models (Hyperparamenter tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is one of the two models with the best accuracy and lowest variance, making it an excellent option for hyperparameter optimization.\n",
    "\n",
    "The logistic regression model's two parameters are as follows:\n",
    "\n",
    "- A solver is an algorithm that aids the model's adaptation to data. There are five forms of logistic regression: liblinear, lbfgs, newton-cg, sag, and saga.\n",
    "\n",
    "- The regularization strength is expressed by the C parameter. A larger number for C indicates more regularization. When the model is supplied with previously unknown data, regularization is synonymous with generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code optimizes the above hyperparameters as well as the count vectorizer parameters IN ORDER TO FIND THE BEST PARAMETERS FOR COUNT VECTORIZER AND LOGISTIC REGRESSION MODEL:\n",
    "\n",
    "- Libraries had previously been imported in the previous code cell, but I opted to re-import them to make this code sample self-contained. In this situation, we simply require sklearn with cross-validation, model deployment, pipelining, and count vectorizer sub-packages.\n",
    "\n",
    "- This time, the pipeline just contains the count vectorizer and the logistic regression model. There is no need for a thick transformer.\n",
    "\n",
    "- Moving on, the parameters list contains all of the potential hyperparameter combinations that the grid_search tries for each component of the pipeline. The parameter max_df of the Count, Vectorizer, for example, is in charge of the model's generalization.\n",
    "\n",
    "- A max_df eliminates words that appear too often, and a max_df of 0.7 removes words that appear too frequently, concretely in more than 70% of the documents. In one scenario vect__max_df will combine a max_df of 0.7 with a ngram_range of (1,1), using a kernel poly and a C parameter of 10. The total “fits” (combinations) are 405 because each cross-validation is performed 5 times.\n",
    "\n",
    "- The final section of code begins reporting the results on the console when calculations are conducted after starting grid_search.fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Pipeline with CountVectorizer and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('LR', LogisticRegression(max_iter = 10000))\n",
    "])\n",
    "\n",
    "# Defining hyperparameters\n",
    "parameters = {\n",
    "    'vect__max_df':[0.7,0.8,0.9],\n",
    "    'vect__ngram_range':  [(1,1), (1,2), (1,3)],\n",
    "    'LR__solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'LR__C': [100, 10, 1.0, 0.1]\n",
    "}\n",
    "\n",
    "# Define grid search\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(pipeline, param_grid=parameters, refit = True, verbose = 3, cv=5)\n",
    "grid_result = grid_search.fit(x_input.values.astype('U'), y_target.values.astype('U'))\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C = 100\n",
    "- type of kernel = liblinear\n",
    "- max_df = 0.7\n",
    "- ngram_range = (1, 3)\n",
    "\n",
    "With the settings, the model achieves a 98% accuracy when deploying the cross-validation method on only 3000 news. The result is already extremely promising. Given the optimal hyperparameters such as C and max_df, we can already tell the generalization of the algorithm should be enough to correctly classify unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Decision Tree Classifier (CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree Classifier (CART) has the highest accuracy of the six methods studied. It should perform even better after hyperparameter adjustment.\n",
    "\n",
    "The CART model has three parameters:\n",
    "\n",
    "- max_features is \"the number of features to consider each time to make the split decision\". If the data collection has 50 columns, you may set this option to 10 to include just 10 of them during the training phase.\n",
    "\n",
    "- max_depth as the greatest number of \"ramifications\" a tree can have is specified by max_depth. A greater number may result in overfitting.\n",
    "\n",
    "- min_samples_leaf is defined as \"the minimum number of samples required to be present at a leaf node\". The value determines how many split points the tree must have.\n",
    "There are no additional parameters than hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Pipeline with CountVectorizer and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('CART', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Defining hyperparameters\n",
    "parameters = {\n",
    "    'vect__max_df':[0.7,0.8,0.9],\n",
    "    'vect__ngram_range':  [(1,1), (1,2), (1,3)],\n",
    "    'CART__max_features': [0.2, 0.4, 0.6, 0.8],\n",
    "    'CART__max_depth': [3,4,5,6],\n",
    "    'CART__min_samples_leaf': [0.04, 0.06, 0.08]\n",
    "}\n",
    "\n",
    "# Define grid search\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(pipeline, param_grid=parameters, refit = True, verbose = 3, cv=5)\n",
    "grid_result = grid_search.fit(x_input.values.astype('U'), y_target.values.astype('U'))\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_depth = 4\n",
    "- max_features = 0.8\n",
    "- min_sample_leaf = 0.04\n",
    "- max_df = 0.7\n",
    "- ngram_range = (1, 1)\n",
    "\n",
    "With the above hyperparameters for the count vectorizer and the algorithm, the average accuracy reaches a 99% score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>>>>>>>>>>>>>>>> NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we deploy the model on the 40.000 records and test it on 9000, the accuracy stays the same. A 99% precision remains a great achievement, on the other hand, though, 1% of approximately 4000 records has been misclassified, and the logistic regression seems overall a better solution for deployment on new records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've finally arrived at the end. As a reminder of what has been accomplished thus far:\n",
    "\n",
    "- We determined that logistic regression is the best model.\n",
    "\n",
    "- A logistic regression model was tweaked, trained on 31.000 data, and tested on the remaining 9000.\n",
    "\n",
    "- This part seeks to determine whether the preceding hypothesis of a common characteristic within the dataset is correct. How do we go about it? We do this by testing the model on a second fake news dataset to see if it produces the same results. The collection has only one limitation: it only contains bogus news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe\n",
    "fake_test = pd.read_csv(DATA_BASE_2_PATH + \"fake.csv\")\n",
    "\n",
    "# Adding classification column to identify fake-news\n",
    "fake_test['classification'] = 1\n",
    "\n",
    "# Selecting relevant columns with only english records\n",
    "fake_test = fake_test[['text', 'language', 'classification']]\n",
    "fake_test = fake_test.query(\"language == 'english'\")\n",
    "\n",
    "# Espliciting input and output variables\n",
    "x_test = fake_test['text']\n",
    "y_test = fake_test['classification']\n",
    "\n",
    "# Transforming 'text' column into bag of words\n",
    "bow_test = count_vect.transform(x_test.values.astype('U'))\n",
    "\n",
    "# Classify new records\n",
    "predicted_LR_test = LR.predict(bow_test)\n",
    "\n",
    "# Print accuracy report\n",
    "print(classification_report(y_test, predicted_LR_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be a number of reasons why a trained model is so accurate, especially in this scenario.\n",
    "\n",
    "- \"Overfitting\" is among them. Overfitting is likely to occur if the model classifies records extremely well within the same dataset but loses accuracy when deployed on fresh data.\n",
    "\n",
    "- A \"lucky pull\" might be another factor. If a model trains on the correct fraction of a dataset and is tested on easy-to-classify records, a fortunate pull may have occurred. Cross-validation, on the other hand, should eradicate the problem at its source.\n",
    "\n",
    "- A third explanation might be that the dataset shares a feature that external data does not. It is probable that all of the phony news in this dataset contains the term or the hashtag “fake”. It would be much easier for a model to understand truthful and fake news within the same dataset, but once presented with new data, accuracy might decrease.\n",
    "\n",
    "A common solution to all of the above would be to deploy the model on a different dataset and assess whether performance stays the same or decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As anticipated, the dataset only contained fake news, and the classifier correctly predicted each one of them, maintaining what had been promised during the tuning and testing phase. Our model classifies with 100% precision whether the news is fake or not, even with an external dataset with minimum computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, considering the algorithm has been deployed on a different dataset, the performance is more than satisfactory. It allows us to eliminate the overfitting and common feature hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
